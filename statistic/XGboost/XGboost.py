# -*- coding: utf-8 -*-
"""
XGBoost Classification Script for ADHD Subtype Comparison (Fixed Version)

This script uses the powerful XGBoost algorithm to classify ADHD subtypes
and compares its performance with other models like SVM.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import xgboost as xgb
import warnings

warnings.filterwarnings('ignore')

# å¤„ç†SHAPå¯¼å…¥å¯èƒ½çš„é”™è¯¯
try:
    import shap

    SHAP_AVAILABLE = True
except ImportError:
    print("âš ï¸ SHAP not available. Feature importance analysis will be skipped.")
    SHAP_AVAILABLE = False

# å¤„ç†imblearnå¯¼å…¥å¯èƒ½çš„é”™è¯¯
try:
    from imblearn.over_sampling import RandomOverSampler
    from imblearn.pipeline import Pipeline as ImbPipeline

    IMBLEARN_AVAILABLE = True
except ImportError:
    print("âš ï¸ imblearn not available. Using sklearn pipeline instead.")
    from sklearn.pipeline import Pipeline as ImbPipeline

    IMBLEARN_AVAILABLE = False

# --- 1. Parameters and Configuration ---
FILE_PATH = 'antç½‘ç»œé›·è¾¾å›¾.xlsx'
SHEET_NAME = 'Sheet1'
FEATURE_COLUMNS = [
    # 'Age', 'Accuracy', 'Response Time', 'Alerting', 'Orienting',
    # 'Executive Control',
    'maxnegpkamp', 'maxnegpkamp_cluster1',
    'maxnegpkamp_cluster2', 'mxdnslp_cluster1', 'mxdnslp_cluster2',
    # 'maxpospkamp', 'mxdnslp', 'mxupslp', 'sw_density', 'mean_duration'
]
GROUP_ADHD1 = 1
GROUP_ADHD3 = 3


# --- 2. Data Loading and Preparation Functions ---
def load_data(file_path, sheet_name):
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        print(f"âœ… Data successfully loaded from '{file_path}'.")
        return df
    except FileNotFoundError:
        print(f"âŒ ERROR: File not found at '{file_path}'. Please check the file path.")
        return None
    except Exception as e:
        print(f"âŒ ERROR loading data: {e}")
        return None


def prepare_data(df, features, target_groups, target_column='Group'):
    df_filtered = df[df[target_column].isin(target_groups)].copy()
    df_filtered.dropna(subset=features + [target_column], inplace=True)
    if len(df_filtered) < 10:
        print(f"âš ï¸ Warning: Not enough data for groups {target_groups}.")
        return None, None
    X = df_filtered[features]
    y = df_filtered[target_column].map({target_groups[0]: 0, target_groups[1]: 1})
    print(f"Data prepared for groups {target_groups}.")
    print(f"Number of samples: {len(X)}. Class distribution:\n{y.value_counts(normalize=True)}")
    return X, y


# --- 3. XGBoost Model Training and Evaluation Function ---
def run_xgboost_classification(X, y, model_name="XGBoost Classification"):
    """
    Trains and evaluates an XGBoost classifier with hyperparameter tuning.
    """
    # --- Train-Test Split ---
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )

    # --- Create XGBoost Classifier with CPU-only configuration ---
    # å…³é”®ä¿®å¤ï¼šç¦ç”¨GPUï¼Œä½¿ç”¨CPUæ¨¡å¼ï¼Œå¹¶è®¾ç½®æ›´ä¿å®ˆçš„å‚æ•°
    xgb_classifier = xgb.XGBClassifier(
        use_label_encoder=False,
        eval_metric='logloss',
        random_state=42,
        tree_method='hist',  # ä½¿ç”¨histogramæ–¹æ³•ï¼Œæ›´ç¨³å®š
        device='cpu',  # å¼ºåˆ¶ä½¿ç”¨CPU
        n_jobs=1,  # ä½¿ç”¨å•çº¿ç¨‹ï¼Œé¿å…å¹¶å‘é—®é¢˜
        verbosity=0  # å‡å°‘è¾“å‡ºä¿¡æ¯
    )

    # --- Create Pipeline ---
    if IMBLEARN_AVAILABLE:
        pipeline = ImbPipeline([
            ('scaler', StandardScaler()),
            ('sampler', RandomOverSampler(random_state=42)),
            ('xgb', xgb_classifier)
        ])
    else:
        pipeline = ImbPipeline([
            ('scaler', StandardScaler()),
            ('xgb', xgb_classifier)
        ])

    # --- ç®€åŒ–çš„è¶…å‚æ•°ç½‘æ ¼ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦ ---
    print("\n--- Starting GridSearchCV for XGBoost ---")
    param_grid = {
        'xgb__n_estimators': [50, 100],
        'xgb__max_depth': [3, 5],
        'xgb__learning_rate': [0.1, 0.2],
        'xgb__subsample': [0.8, 1.0]
    }

    try:
        grid_search = GridSearchCV(
            pipeline,
            param_grid,
            cv=3,  # å‡å°‘äº¤å‰éªŒè¯æŠ˜æ•°
            scoring='f1_macro',
            n_jobs=1,  # ä½¿ç”¨å•çº¿ç¨‹
            verbose=1
        )
        grid_search.fit(X_train, y_train)

        # --- Get the Best Model ---
        print("\n--- GridSearchCV Complete ---")
        print(f"Best parameters found: {grid_search.best_params_}")
        print(f"Best cross-validation F1-macro score: {grid_search.best_score_:.3f}")

        best_model = grid_search.best_estimator_

    except Exception as e:
        print(f"âŒ GridSearchCV failed: {e}")
        print("ğŸ”„ Using default parameters instead...")

        # å¦‚æœç½‘æ ¼æœç´¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
        pipeline.fit(X_train, y_train)
        best_model = pipeline

    # --- Predictions ---
    try:
        y_pred = best_model.predict(X_test)
        y_pred_proba = best_model.predict_proba(X_test)[:, 1]
    except Exception as e:
        print(f"âŒ Prediction failed: {e}")
        return

    # --- Evaluation on Test Set ---
    print("\n--- Test Set Evaluation Results (with XGBoost) ---")
    print(classification_report(y_test, y_pred, zero_division=0))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # AUC Score
    try:
        auc_score = roc_auc_score(y_test, y_pred_proba)
        print(f"AUC Score: {auc_score:.3f}")
    except Exception as e:
        print(f"âš ï¸ Could not calculate AUC: {e}")

    # --- Visualization ---
    try:
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='Chance Level (AUC = 0.5)')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve for {model_name}')
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    except Exception as e:
        print(f"âš ï¸ Could not create ROC curve: {e}")

    # --- Feature Importance (XGBoost built-in) ---
    try:
        print("\n--- Feature Importance Analysis ---")
        xgb_model = best_model.named_steps['xgb']
        feature_importance = xgb_model.feature_importances_

        # Create feature importance plot
        feature_names = X.columns
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)

        plt.figure(figsize=(10, 8))
        plt.barh(range(len(importance_df)), importance_df['importance'])
        plt.yticks(range(len(importance_df)), importance_df['feature'])
        plt.xlabel('Feature Importance')
        plt.title('XGBoost Feature Importance')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

        print("Top 5 Most Important Features:")
        print(importance_df.head())

    except Exception as e:
        print(f"âš ï¸ Could not generate feature importance: {e}")

    # --- SHAP Value Analysis (if available) ---
    if SHAP_AVAILABLE:
        try:
            print("\n--- Calculating SHAP Values ---")
            # è·å–XGBoostæ¨¡å‹
            xgb_model = best_model.named_steps['xgb']

            # å‡†å¤‡æ•°æ®
            if IMBLEARN_AVAILABLE:
                X_test_scaled = best_model.named_steps['scaler'].transform(X_test)
                # éœ€è¦åº”ç”¨é‡‡æ ·å™¨æ¥è·å¾—æ­£ç¡®çš„ç‰¹å¾
                X_test_resampled, _ = best_model.named_steps['sampler'].fit_resample(X_test_scaled, y_test)
                X_for_shap = X_test_resampled[:len(X_test)]  # å–åŸå§‹æµ‹è¯•é›†å¤§å°
            else:
                X_for_shap = best_model.named_steps['scaler'].transform(X_test)

            # åˆ›å»ºSHAPè§£é‡Šå™¨
            explainer = shap.TreeExplainer(xgb_model)
            shap_values = explainer.shap_values(X_for_shap)

            # ç”ŸæˆSHAPæ‘˜è¦å›¾
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values, X_for_shap, feature_names=X.columns, show=False)
            plt.title(f"SHAP Feature Importance for {model_name}")
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"âš ï¸ SHAP analysis failed: {e}")
    else:
        print("âš ï¸ SHAP not available for advanced feature analysis.")


# --- 4. Main Execution Block ---
if __name__ == "__main__":
    print("ğŸš€ Starting XGBoost Classification Analysis")
    print("=" * 60)

    # Load data
    df_main = load_data(FILE_PATH, SHEET_NAME)
    if df_main is not None:
        print(f"\nğŸ“Š Dataset shape: {df_main.shape}")
        print(f"ğŸ“‹ Available columns: {list(df_main.columns)}")

        # Check if required columns exist
        missing_cols = [col for col in FEATURE_COLUMNS if col not in df_main.columns]
        if missing_cols:
            print(f"âš ï¸ Missing columns: {missing_cols}")
            print("Available columns in dataset:")
            print(df_main.columns.tolist())

            # Use only available columns
            available_features = [col for col in FEATURE_COLUMNS if col in df_main.columns]
            if available_features:
                print(f"ğŸ”„ Using available features: {available_features}")
                FEATURE_COLUMNS = available_features
            else:
                print("âŒ No valid feature columns found.")
                exit(1)

        print("\n" + "=" * 60)
        print("ğŸ¯ Running XGBoost for: ADHD-I vs. ADHD-C")
        print("=" * 60)

        X, y = prepare_data(df_main, FEATURE_COLUMNS, [GROUP_ADHD1, GROUP_ADHD3], target_column='Group')
        if X is not None and y is not None:
            run_xgboost_classification(X, y)
        else:
            print("âŒ Skipping analysis due to insufficient data.")
    else:
        print("âŒ Failed to load data. Please check the file path and format.")

    print("\nâœ… All tasks complete.")