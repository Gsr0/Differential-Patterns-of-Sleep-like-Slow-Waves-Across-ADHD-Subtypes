# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆADHDåˆ†ç±»æ¨¡å‹ - ç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹è°ƒä¼˜
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV, SelectKBest, f_classif
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import xgboost as xgb
from scipy import stats
import shap

# --- 1. å‚æ•°é…ç½® ---
FILE_PATH = 'antç½‘ç»œé›·è¾¾å›¾.xlsx'
SHEET_NAME = 'Sheet1'
GROUP_HC = 0
GROUP_ADHD1 = 1
GROUP_ADHD3 = 3

# æ‰€æœ‰å¯èƒ½ç‰¹å¾ï¼ˆæ ¹æ®æ‚¨çš„æè¿°é€‰æ‹©ï¼‰
FEATURE_CANDIDATES = [
    # 'Age',  # å¹´é¾„
    # 'Accuracy',  # è¡Œä¸ºå­¦ - å‡†ç¡®ç‡ (ACC)
    # 'Response Time',  # è¡Œä¸ºå­¦ - ååº”æ—¶ (RT)
    # 'Alerting',  # ANT - è­¦è§‰ç½‘ç»œ
    # 'Orienting',  # ANT - å®šå‘ç½‘ç»œ
    # 'Executive Control',  # ANT - æ‰§è¡Œæ§åˆ¶ç½‘ç»œ
    # 'maxnegpkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§è´Ÿæ³¢å¹…
    # 'maxnegpkamp_1',
    # 'maxnegpkamp_3',
    # 'maxnegpkamp_5',
    # 'maxnegpkamp_7',
    # 'maxnegpkamp_9',
    # 'maxnegpkamp_10',
    # 'maxnegpkamp_12',
    'maxnegpkamp_cluster1',
    # 'maxnegpkamp_cluster2',
    # 'maxnegpkamp_cluster',
    # 'maxnegpkamp/mxdnslp_1',
    # 'mxdnslp_1',
    # 'mxdnslp_3',
    # 'mxdnslp_4',
    # 'mxdnslp_5',
    # 'mxdnslp_7',
    # 'mxdnslp_8',
    # 'mxdnslp_9',
    # 'mxdnslp_12',
    # 'mxdnslp_14',
    # 'mxdnslp_18',
    # 'mxdnslp_cluster1',
    # 'mxdnslp_cluster2',
    'mxdnslp_cluster',
    'mxupslp_cluster',
    # 'mxupslp_1',
    # 'mxupslp_3',
    # 'mxupslp_7',
    # 'mxupslp_5',
    # 'maxpospkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§æ­£æ³¢å¹…
    # 'mxdnslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸‹é™æ–œç‡
    # 'mxupslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸Šå‡æ–œç‡
    # 'sw_density',  # æ…¢æ³¢å‚æ•° - æ…¢æ³¢å¯†åº¦
    # 'mean_duration'  # æ…¢æ³¢å‚æ•° - å¹³å‡æŒç»­æ—¶é—´
]


# --- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†å‡½æ•° ---
def load_and_preprocess_data(file_path, sheet_name):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        print(f"âœ… æ•°æ®æˆåŠŸåŠ è½½: '{file_path}'")
        print(f"æ€»æ ·æœ¬æ•°: {len(df)}")

        # å¤„ç†ç¼ºå¤±å€¼
        df.dropna(subset=FEATURE_CANDIDATES + ['Group'], inplace=True)

        # åˆ›å»ºADHDäºŒå…ƒæ ‡ç­¾
        df['ADHD_binary'] = df['Group'].apply(lambda x: 0 if x == GROUP_HC else 1)

        return df
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é”™è¯¯: {e}")
        return None


# --- 3. ç‰¹å¾å·¥ç¨‹å‡½æ•° ---
def feature_engineering(X, y):
    """æ‰§è¡Œç‰¹å¾å·¥ç¨‹"""
    # 1. å¼‚å¸¸å€¼å¤„ç† - Winsorization
    for col in X.columns:
        X[col] = stats.mstats.winsorize(X[col], limits=[0.05, 0.05])

    # 2. ç‰¹å¾å˜æ¢ - å¯¹æ•°å˜æ¢
    transformer = PowerTransformer(method='yeo-johnson')
    X_transformed = transformer.fit_transform(X)
    X = pd.DataFrame(X_transformed, columns=X.columns)

    return X, y


# --- 4. ç‰¹å¾é€‰æ‹©å‡½æ•° ---
def select_features(X, y):
    """ä½¿ç”¨å¤šç§æ–¹æ³•é€‰æ‹©æœ€ä½³ç‰¹å¾"""
    print("\n--- ç‰¹å¾é€‰æ‹© ---")

    # æ–¹æ³•1: éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)

    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(10, 6))
    feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
    feat_importances.nlargest(10).plot(kind='barh')
    plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§')
    plt.tight_layout()
    plt.show()

    # æ–¹æ³•2: RFECV (é€’å½’ç‰¹å¾æ¶ˆé™¤äº¤å‰éªŒè¯)
    svm = SVC(kernel="linear", random_state=42)
    rfecv = RFECV(
        estimator=svm,
        step=1,
        cv=StratifiedKFold(5),
        scoring='accuracy',
        min_features_to_select=3
    )
    rfecv.fit(X, y)

    print(f"RFECVé€‰æ‹©çš„æœ€ä½³ç‰¹å¾æ•°: {rfecv.n_features_}")
    selected_features = X.columns[rfecv.support_]
    print(f"RFECVé€‰æ‹©çš„ç‰¹å¾: {list(selected_features)}")

    # æ–¹æ³•3: ç›¸å…³æ€§åˆ†æ
    corr_matrix = X.corr()
    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
    plt.tight_layout()
    plt.show()

    # ç»¼åˆé€‰æ‹©ç‰¹å¾ - è¿™é‡Œä½¿ç”¨RFECVçš„ç»“æœ
    return X[selected_features], selected_features


# --- 5. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ---
def train_and_evaluate(X, y, model_name="ADHDåˆ†ç±»æ¨¡å‹"):
    """è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹"""
    print(f"\n--- è®­ç»ƒæ¨¡å‹: {model_name} ---")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # åˆ›å»ºæ¨¡å‹ç®¡é“
    pipeline = ImbPipeline([
        ('scaler', StandardScaler()),
        ('sampler', SMOTE(random_state=42)),  # ä½¿ç”¨SMOTEå¤„ç†ä¸å¹³è¡¡
        ('classifier', SVC(probability=True, random_state=42))
    ])

    # å®šä¹‰å‚æ•°ç½‘æ ¼ - åŒ…å«å¤šç§æ¨¡å‹
    param_grid = [
        {
            'classifier': [SVC(probability=True, random_state=42)],
            'classifier__C': [0.1, 1, 10, 100],
            'classifier__kernel': ['linear', 'rbf', 'poly'],
            'classifier__gamma': ['scale', 'auto', 0.1, 1],
            'classifier__class_weight': [None, 'balanced']
        },
        {
            'classifier': [xgb.XGBClassifier(
                # use_label_encoder=False,
                eval_metric='logloss',
                random_state=42,
                tree_method='hist',
                device='cpu'
            )],
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [3, 5, 7],
            'classifier__learning_rate': [0.01, 0.1],
            'classifier__subsample': [0.8, 1.0],
            'classifier__colsample_bytree': [0.8, 1.0]
        }
    ]

    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(
        pipeline,
        param_grid,
        cv=StratifiedKFold(5),
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )

    print("æ­£åœ¨è¿›è¡Œç½‘æ ¼æœç´¢...")
    grid_search.fit(X_train, y_train)

    # è¾“å‡ºæœ€ä½³å‚æ•°
    print("\næœ€ä½³å‚æ•°:")
    print(grid_search.best_params_)

    # è·å–æœ€ä½³æ¨¡å‹
    best_model = grid_search.best_estimator_

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1]  # æ­£ç±»çš„æ¦‚ç‡

    print("\næµ‹è¯•é›†æ€§èƒ½:")
    print(classification_report(y_test, y_pred))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.show()

    # ROCæ›²çº¿
    roc_auc = roc_auc_score(y_test, y_proba)
    print(f"ROC AUC åˆ†æ•°: {roc_auc:.3f}")

    RocCurveDisplay.from_estimator(best_model, X_test, y_test)
    plt.title(f'{model_name} - ROCæ›²çº¿ (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.tight_layout()
    plt.show()

    # SHAPè§£é‡Šï¼ˆå¦‚æœæ˜¯æ ‘æ¨¡å‹ï¼‰
    if 'XGBClassifier' in str(type(best_model.named_steps['classifier'])):
        print("\nç”ŸæˆSHAPè§£é‡Š...")
        explainer = shap.TreeExplainer(best_model.named_steps['classifier'])
        X_test_scaled = best_model.named_steps['scaler'].transform(X_test)
        shap_values = explainer.shap_values(X_test_scaled)

        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, show=False)
        plt.title('SHAPç‰¹å¾é‡è¦æ€§')
        plt.tight_layout()
        plt.show()

    return best_model, grid_search.best_score_


# --- 6. ä¸»æ‰§è¡Œæ¨¡å— ---
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    df = load_and_preprocess_data(FILE_PATH, SHEET_NAME)

    if df is not None:
        # ===================================================================
        # ä»»åŠ¡1: HC vs ADHD
        # ===================================================================
        print("\n" + "=" * 50)
        print("ğŸš€ ä»»åŠ¡1: å¥åº·å¯¹ç…§ç»„ vs ADHDæ‚£è€…")
        print("=" * 50)

        # å‡†å¤‡æ•°æ®
        X1 = df[FEATURE_CANDIDATES]
        y1 = df['ADHD_binary']

        # ç‰¹å¾å·¥ç¨‹
        X1, y1 = feature_engineering(X1, y1)

        # ç‰¹å¾é€‰æ‹©
        X1_selected, selected_features = select_features(X1, y1)

        # è®­ç»ƒæ¨¡å‹
        model1, cv_score1 = train_and_evaluate(X1_selected, y1, "HC vs ADHD")

        # ===================================================================
        # ä»»åŠ¡2: ADHD-I vs ADHD-C
        # ===================================================================
        print("\n" + "=" * 50)
        print("ğŸš€ ä»»åŠ¡2: ADHD-I vs ADHD-C äºšå‹åˆ†ç±»")
        print("=" * 50)

        # ç­›é€‰ADHDæ‚£è€…
        adhd_df = df[df['ADHD_binary'] == 1]

        # å‡†å¤‡æ•°æ®
        X2 = adhd_df[FEATURE_CANDIDATES]
        y2 = adhd_df['Group'].map({GROUP_ADHD1: 0, GROUP_ADHD3: 1})

        # ç‰¹å¾å·¥ç¨‹
        X2, y2 = feature_engineering(X2, y2)

        # ç‰¹å¾é€‰æ‹©
        X2_selected, _ = select_features(X2, y2)

        # è®­ç»ƒæ¨¡å‹
        model2, cv_score2 = train_and_evaluate(X2_selected, y2, "ADHD-I vs ADHD-C")

        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 50)
        print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
        print("=" * 50)
        print(f"ä»»åŠ¡1: HC vs ADHD - äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_score1:.4f}")
        print(f"ä»»åŠ¡2: ADHD-I vs ADHD-C - äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_score2:.4f}")

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")