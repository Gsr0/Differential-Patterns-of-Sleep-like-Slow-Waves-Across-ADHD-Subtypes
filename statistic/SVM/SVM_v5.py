# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆADHDäºšå‹åˆ†ç±»æ¨¡å‹ - åŸºäºæ…¢æ³¢ç‰¹å¾å’Œè„‘åŒºç‰¹å¼‚æ€§ (ä¸ä½¿ç”¨skopt)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_selection import RFECV, SelectKBest, f_classif, mutual_info_classif
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay, make_scorer
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import xgboost as xgb
from scipy import stats
import shap
import joblib
from scipy.stats import uniform, randint

# --- 1. å‚æ•°é…ç½® ---
FILE_PATH = 'antç½‘ç»œé›·è¾¾å›¾.xlsx'
SHEET_NAME = 'Sheet1'
GROUP_HC = 0
GROUP_ADHD1 = 1
GROUP_ADHD3 = 3

# æ‰€æœ‰å¯èƒ½ç‰¹å¾ï¼ˆæ ¹æ®æ‚¨çš„ç ”ç©¶é€‰æ‹©ï¼‰
FRONTAL_ELECTRODES = [1, 3, 5]  # é¢å¶ç”µæ
ALL_FEATURES = [
    'maxnegpkamp', 'mxdnslp', 'mxupslp',
]

# ç”Ÿæˆç‰¹å¾åˆ—å
FEATURE_COLUMNS = []
for feature in ALL_FEATURES:
    for electrode in FRONTAL_ELECTRODES:
        FEATURE_COLUMNS.append(f"{feature}_{electrode}")

print("ä½¿ç”¨çš„ç‰¹å¾:", FEATURE_COLUMNS)


# --- 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†å‡½æ•° ---
def load_and_preprocess_data(file_path, sheet_name):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        print(f"âœ… æ•°æ®æˆåŠŸåŠ è½½: '{file_path}'")
        print(f"æ€»æ ·æœ¬æ•°: {len(df)}")

        # å¤„ç†ç¼ºå¤±å€¼
        df.dropna(subset=FEATURE_COLUMNS + ['Group'], inplace=True)

        # åˆ›å»ºADHDäºŒå…ƒæ ‡ç­¾
        df['ADHD_binary'] = df['Group'].apply(lambda x: 0 if x == GROUP_HC else 1)

        # æ·»åŠ ç‰¹å¾å·¥ç¨‹ï¼šåˆ›å»ºé¢å¶åŒºåŸŸç‰¹å¾
        for feature in ALL_FEATURES:
            frontal_cols = [f"{feature}_{e}" for e in FRONTAL_ELECTRODES]
            df[f'frontal_{feature}_mean'] = df[frontal_cols].mean(axis=1)
            df[f'frontal_{feature}_std'] = df[frontal_cols].std(axis=1)
            FEATURE_COLUMNS.extend([f'frontal_{feature}_mean', f'frontal_{feature}_std'])

        return df
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é”™è¯¯: {e}")
        return None


# --- 3. ç‰¹å¾å·¥ç¨‹å‡½æ•° ---
def feature_engineering(X, y):
    """æ‰§è¡Œç‰¹å¾å·¥ç¨‹"""
    # 1. å¼‚å¸¸å€¼å¤„ç† - Winsorization
    for col in X.columns:
        X[col] = stats.mstats.winsorize(X[col], limits=[0.05, 0.05])

    # 2. ç‰¹å¾å˜æ¢ - å¯¹æ•°å˜æ¢
    transformer = PowerTransformer(method='yeo-johnson')
    X_transformed = transformer.fit_transform(X)
    X = pd.DataFrame(X_transformed, columns=X.columns)

    # 3. åˆ›å»ºäº¤äº’ç‰¹å¾ï¼ˆé¢å¶ç”µæé—´çš„æ¯”å€¼ï¼‰
    for feature in ALL_FEATURES:
        for i in range(len(FRONTAL_ELECTRODES)):
            for j in range(i + 1, len(FRONTAL_ELECTRODES)):
                e1 = FRONTAL_ELECTRODES[i]
                e2 = FRONTAL_ELECTRODES[j]
                ratio_col = f"{feature}_ratio_{e1}_{e2}"
                X[ratio_col] = X[f"{feature}_{e1}"] / (X[f"{feature}_{e2}"] + 1e-6)

    return X, y


# --- 4. ç‰¹å¾é€‰æ‹©å‡½æ•° ---
def select_features(X, y):
    """ä½¿ç”¨å¤šç§æ–¹æ³•é€‰æ‹©æœ€ä½³ç‰¹å¾"""
    print("\n--- ç‰¹å¾é€‰æ‹© ---")

    # æ–¹æ³•1: éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X, y)

    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(12, 8))
    feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
    feat_importances.nlargest(15).plot(kind='barh')
    plt.title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§')
    plt.tight_layout()
    plt.show()

    # æ–¹æ³•2: äº’ä¿¡æ¯
    mi_scores = mutual_info_classif(X, y)
    mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores})
    mi_df = mi_df.sort_values('MI_Score', ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='MI_Score', y='Feature', data=mi_df.head(15))
    plt.title('äº’ä¿¡æ¯ç‰¹å¾é‡è¦æ€§')
    plt.tight_layout()
    plt.show()

    # æ–¹æ³•3: RFECV (é€’å½’ç‰¹å¾æ¶ˆé™¤äº¤å‰éªŒè¯)
    svm = SVC(kernel="linear", random_state=42)
    rfecv = RFECV(
        estimator=svm,
        step=1,
        cv=StratifiedKFold(5),
        scoring='accuracy',
        min_features_to_select=5
    )
    rfecv.fit(X, y)

    print(f"RFECVé€‰æ‹©çš„æœ€ä½³ç‰¹å¾æ•°: {rfecv.n_features_}")
    selected_features = X.columns[rfecv.support_]
    print(f"RFECVé€‰æ‹©çš„ç‰¹å¾: {list(selected_features)}")

    # å¯è§†åŒ–RFECVç»“æœ
    plt.figure()
    plt.xlabel("ç‰¹å¾æ•°é‡")
    plt.ylabel("äº¤å‰éªŒè¯å‡†ç¡®ç‡")
    plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'])
    plt.title('RFECVç»“æœ')
    plt.tight_layout()
    plt.show()

    # ç»¼åˆé€‰æ‹©ç‰¹å¾
    return X[selected_features], selected_features


# --- 5. è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ ---
def specificity_score(y_true, y_pred):
    """è®¡ç®—ç‰¹å¼‚æ€§ï¼ˆçœŸé˜´æ€§ç‡ï¼‰"""
    cm = confusion_matrix(y_true, y_pred)
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
        return tn / (tn + fp) if (tn + fp) > 0 else 0
    return 0


# --- 6. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ---
def train_and_evaluate(X, y, model_name="ADHDäºšå‹åˆ†ç±»æ¨¡å‹"):
    """è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹"""
    print(f"\n--- è®­ç»ƒæ¨¡å‹: {model_name} ---")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # åˆ›å»ºè‡ªå®šä¹‰è¯„åˆ†å™¨
    specificity_scorer = make_scorer(specificity_score)

    # å®šä¹‰å‚æ•°åˆ†å¸ƒï¼ˆä½¿ç”¨RandomizedSearchCVæ›¿ä»£BayesSearchCVï¼‰
    param_dist = [
        {
            'classifier': [SVC(probability=True, random_state=42)],
            'classifier__C': uniform(0.001, 1000),  # å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
            'classifier__kernel': ['linear', 'rbf', 'poly'],
            'classifier__gamma': ['scale', 'auto'] + list(uniform(0.0001, 10).rvs(10)),
            'classifier__class_weight': [None, 'balanced']
        },
        {
            'classifier': [xgb.XGBClassifier(
                # use_label_encoder=False,
                eval_metric='logloss',
                random_state=42,
                tree_method='hist',
                device='cpu'
            )],
            'classifier__n_estimators': randint(50, 300),
            'classifier__max_depth': randint(3, 10),
            'classifier__learning_rate': uniform(0.001, 0.3),  # å¯¹æ•°å‡åŒ€åˆ†å¸ƒ
            'classifier__subsample': uniform(0.6, 0.4),  # 0.6-1.0
            'classifier__colsample_bytree': uniform(0.6, 0.4),  # 0.6-1.0
            'classifier__gamma': uniform(0, 5),
            'classifier__reg_alpha': uniform(0, 10),
            'classifier__reg_lambda': uniform(1, 9)  # 1-10
        }
    ]

    # åˆ›å»ºæ¨¡å‹ç®¡é“
    pipeline = ImbPipeline([
        ('scaler', StandardScaler()),
        ('sampler', SMOTE(random_state=42)),  # ä½¿ç”¨SMOTEå¤„ç†ä¸å¹³è¡¡
        ('classifier', SVC(probability=True, random_state=42))
    ])

    # éšæœºæœç´¢ä¼˜åŒ–
    random_search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_dist,
        n_iter=32,  # è¿­ä»£æ¬¡æ•°
        cv=StratifiedKFold(5),
        scoring='accuracy',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )

    print("æ­£åœ¨è¿›è¡Œéšæœºæœç´¢ä¼˜åŒ–...")
    random_search.fit(X_train, y_train)

    # è¾“å‡ºæœ€ä½³å‚æ•°
    print("\næœ€ä½³å‚æ•°:")
    print(random_search.best_params_)

    # è·å–æœ€ä½³æ¨¡å‹
    best_model = random_search.best_estimator_

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1]  # æ­£ç±»çš„æ¦‚ç‡

    print("\næµ‹è¯•é›†æ€§èƒ½:")
    print(classification_report(y_test, y_pred))

    # è®¡ç®—ç‰¹å¼‚æ€§
    specificity = specificity_score(y_test, y_pred)
    print(f"ç‰¹å¼‚æ€§: {specificity:.3f}")

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['ADHD-I', 'ADHD-C'],
                yticklabels=['ADHD-I', 'ADHD-C'])
    plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.show()

    # ROCæ›²çº¿
    roc_auc = roc_auc_score(y_test, y_proba)
    print(f"ROC AUC åˆ†æ•°: {roc_auc:.3f}")

    RocCurveDisplay.from_estimator(best_model, X_test, y_test)
    plt.title(f'{model_name} - ROCæ›²çº¿ (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.tight_layout()
    plt.show()

    # SHAPè§£é‡Š
    try:
        if 'XGBClassifier' in str(type(best_model.named_steps['classifier'])):
            print("\nç”ŸæˆSHAPè§£é‡Š...")
            explainer = shap.TreeExplainer(best_model.named_steps['classifier'])
            X_test_scaled = best_model.named_steps['scaler'].transform(X_test)
            shap_values = explainer.shap_values(X_test_scaled)

            plt.figure(figsize=(12, 8))
            shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, show=False)
            plt.title('SHAPç‰¹å¾é‡è¦æ€§')
            plt.tight_layout()
            plt.show()

            # å•ä¸ªæ ·æœ¬è§£é‡Š
            plt.figure(figsize=(12, 6))
            shap.force_plot(explainer.expected_value, shap_values[0], X_test_scaled[0], feature_names=X.columns,
                            matplotlib=True)
            plt.title('å•ä¸ªæ ·æœ¬SHAPè§£é‡Š')
            plt.tight_layout()
            plt.show()
    except Exception as e:
        print(f"SHAPè§£é‡Šé”™è¯¯: {e}")

    # ä¿å­˜æ¨¡å‹
    joblib.dump(best_model, f'{model_name.replace(" ", "_")}_model.pkl')
    print(f"æ¨¡å‹å·²ä¿å­˜ä¸º '{model_name.replace(' ', '_')}_model.pkl'")

    return best_model, random_search.best_score_


# --- 7. ä¸»æ‰§è¡Œæ¨¡å— ---
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    df = load_and_preprocess_data(FILE_PATH, SHEET_NAME)

    if df is not None:
        # ===================================================================
        # ä»»åŠ¡2: ADHD-I vs ADHD-C
        # ===================================================================
        print("\n" + "=" * 50)
        print("ğŸš€ ä»»åŠ¡2: ADHD-I vs ADHD-C äºšå‹åˆ†ç±»")
        print("=" * 50)

        # ç­›é€‰ADHDæ‚£è€…
        adhd_df = df[df['ADHD_binary'] == 1]

        # å‡†å¤‡æ•°æ®
        X = adhd_df[FEATURE_COLUMNS]
        y = adhd_df['Group'].map({GROUP_ADHD1: 0, GROUP_ADHD3: 1})

        # æ£€æŸ¥æ ·æœ¬åˆ†å¸ƒ
        print(f"ADHD-Iæ ·æœ¬æ•°: {sum(y == 0)}")
        print(f"ADHD-Cæ ·æœ¬æ•°: {sum(y == 1)}")

        # ç‰¹å¾å·¥ç¨‹
        X, y = feature_engineering(X, y)

        # ç‰¹å¾é€‰æ‹©
        X_selected, selected_features = select_features(X, y)

        # è®­ç»ƒæ¨¡å‹
        model, cv_score = train_and_evaluate(X_selected, y, "ADHD-I vs ADHD-C")

        # æœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 50)
        print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
        print("=" * 50)
        print(f"ADHDäºšå‹åˆ†ç±»äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_score:.4f}")

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")