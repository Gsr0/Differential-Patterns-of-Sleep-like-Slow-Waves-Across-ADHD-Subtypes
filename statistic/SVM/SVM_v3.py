# -*- coding: utf-8 -*-
"""
SVM Classification Script for ADHD Slow Wave and Behavioral Data
This script performs SVM classification to distinguish between different participant groups
based on global slow wave parameters and behavioral data.

Tasks performed:
1. Load data from an Excel file.
2. Preprocess data for specific comparison groups.
3. Train and evaluate an SVM model for:
    a) Healthy Controls (HC) vs. All ADHD participants
    b) ADHD-I vs. ADHD-C subtypes
4. NEW: Perform feature selection to find the most effective features for subtype classification.
5. NEW: Calculate and visualize SHAP values to explain feature contributions.
6. Report key performance metrics including accuracy, precision, recall, F1-score,
   confusion matrix, and ROC curve.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay
# NEW: Import RFE for feature selection and the shap library
from sklearn.feature_selection import RFE
import shap

# --- 1. Parameters and Configuration (å‚æ•°è®¾ç½®) ---

# TODO: Fill in your file path and sheet name
# TODO: è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ çš„Excelæ–‡ä»¶åå’Œå·¥ä½œè¡¨å
FILE_PATH = 'antç½‘ç»œé›·è¾¾å›¾.xlsx'  # <-- ä¿®æ”¹è¿™é‡Œ
SHEET_NAME = 'Sheet1'  # <-- ä¿®æ”¹è¿™é‡Œ (å¦‚æœéœ€è¦)

# Define the feature columns to be used in the model
# å®šä¹‰æ¨¡å‹è¦ä½¿ç”¨çš„ç‰¹å¾åˆ—
FEATURE_COLUMNS = [
    # 'Age',  # å¹´é¾„
    # 'Accuracy',  # è¡Œä¸ºå­¦ - å‡†ç¡®ç‡ (ACC)
    # 'Response Time',  # è¡Œä¸ºå­¦ - ååº”æ—¶ (RT)
    # 'Alerting',  # ANT - è­¦è§‰ç½‘ç»œ
    # 'Orienting',  # ANT - å®šå‘ç½‘ç»œ
    # 'Executive Control',  # ANT - æ‰§è¡Œæ§åˆ¶ç½‘ç»œ
    # 'maxnegpkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§è´Ÿæ³¢å¹…
    # 'maxnegpkamp_1',
    # 'maxnegpkamp_3',
    # 'maxnegpkamp_5',
    # 'maxnegpkamp_7',
    # 'maxnegpkamp_9',
    # 'maxnegpkamp_10',
    # 'maxnegpkamp_12',
    'maxnegpkamp_cluster1',
    # 'maxnegpkamp_cluster2',
    # 'maxnegpkamp_cluster',
    # 'maxnegpkamp/mxdnslp_1',
    # 'mxdnslp_1',
    # 'mxdnslp_3',
    # 'mxdnslp_4',
    # 'mxdnslp_5',
    # 'mxdnslp_7',
    # 'mxdnslp_8',
    # 'mxdnslp_9',
    # 'mxdnslp_12',
    # 'mxdnslp_14',
    # 'mxdnslp_18',
    'mxdnslp_cluster1',
    # 'mxdnslp_cluster2',
    # 'mxdnslp_cluster',
    # 'mxupslp_cluster',
    # 'mxupslp_1',
    # 'mxupslp_3',
    # 'mxupslp_7',
    # 'mxupslp_5',
    # 'maxpospkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§æ­£æ³¢å¹…
    # 'mxdnslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸‹é™æ–œç‡
    # 'mxupslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸Šå‡æ–œç‡
    # 'sw_density',  # æ…¢æ³¢å‚æ•° - æ…¢æ³¢å¯†åº¦
    # 'mean_duration'  # æ…¢æ³¢å‚æ•° - å¹³å‡æŒç»­æ—¶é—´
]
# Define group IDs as they appear in your data file
# å®šä¹‰æ•°æ®æ–‡ä»¶ä¸­çš„ç»„åˆ«ID
GROUP_HC = 0  # å¥åº·å„¿ç«¥ (Healthy Controls)
GROUP_ADHD1 = 1  # ADHD-I å‹
GROUP_ADHD3 = 3  # ADHD-C å‹ (å‡è®¾æ··åˆå‹æ˜¯3, è¯·æ ¹æ®ä½ çš„æ•°æ®ä¿®æ”¹)


# --- 2. Data Loading and Preparation Functions (æ•°æ®åŠ è½½ä¸å‡†å¤‡å‡½æ•°) ---

def load_data(file_path, sheet_name):
    """Loads data from the specified Excel file."""
    # """ä»æŒ‡å®šçš„Excelæ–‡ä»¶åŠ è½½æ•°æ®ã€‚"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        print(f"âœ… Data successfully loaded from '{file_path}'.")
        print(f"Total participants in file: {len(df)}")
        return df
    except FileNotFoundError:
        print(f"âŒ ERROR: File not found at '{file_path}'. Please check the file path.")
        return None


def prepare_data(df, features, target_groups, target_column='Group'):
    """
    Prepares data for a specific classification task by filtering groups
    and separating features (X) from the target (y).
    """
    # """ä¸ºç‰¹å®šåˆ†ç±»ä»»åŠ¡å‡†å¤‡æ•°æ®ï¼šç­›é€‰ç»„åˆ«ï¼Œå¹¶åˆ†ç¦»ç‰¹å¾(X)å’Œç›®æ ‡(y)ã€‚"""

    # Filter for the groups of interest
    df_filtered = df[df[target_column].isin(target_groups)].copy()

    # Handle missing values by dropping rows with any missing data in the required columns
    df_filtered.dropna(subset=features + [target_column], inplace=True)

    if len(df_filtered) < 10:  # Check if there's enough data
        print(f"âš ï¸ Warning: Not enough data for groups {target_groups}. Found only {len(df_filtered)} samples.")
        return None, None

    # Define features (X) and target (y)
    X = df_filtered[features]
    y = df_filtered[target_column]

    # Map group labels to binary 0 and 1 for SVM
    # å°†ç»„åˆ«æ ‡ç­¾æ˜ å°„ä¸º0å’Œ1ï¼Œä»¥ä¾›SVMä½¿ç”¨
    y = y.map({target_groups[0]: 0, target_groups[1]: 1})

    print(f"Data prepared for groups {target_groups}.")
    print(f"Original features available: {len(X.columns)}")
    print(f"Number of samples: {len(X)}. Class distribution:\n{y.value_counts()}")

    return X, y


# --- 3. SVM Model Training and Evaluation Function (SVMæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°å‡½æ•°) ---

# MODIFIED: Function now includes feature selection and SHAP analysis capabilities
def run_svm_classification(X, y, model_name="SVM Classification", perform_feature_selection=False,
                           n_features_to_select=8):
    """
    Trains an SVM classifier, evaluates it, and visualizes the results.
    Optionally performs Recursive Feature Elimination (RFE) and SHAP analysis.
    """
    # """è®­ç»ƒä¸€ä¸ªSVMåˆ†ç±»å™¨ï¼Œè¿›è¡Œè¯„ä¼°å’Œå¯è§†åŒ–ã€‚å¯é€‰æ‰§è¡ŒRFEç‰¹å¾é€‰æ‹©å’ŒSHAPåˆ†æã€‚"""

    # --- Train-Test Split (First Step) ---
    # Split data before any scaling or feature selection to prevent data leakage.
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y  # Using 25% test size
    )

    # --- Feature Scaling (ç‰¹å¾ç¼©æ”¾) ---
    # Scale data based on the training set
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Keep track of the feature names being used
    current_feature_names = X.columns.tolist()

    # --- NEW: Feature Selection using RFE (ç‰¹å¾é€‰æ‹©) ---
    if perform_feature_selection:
        print("\n--- Performing Recursive Feature Elimination (RFE) ---")
        # The estimator used by RFE
        estimator = SVC(kernel='linear')
        # The RFE selector
        selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)

        # Fit RFE on the scaled training data
        selector = selector.fit(X_train_scaled, y_train)

        # Get the names of the selected features
        selected_mask = selector.support_
        current_feature_names = X.columns[selected_mask].tolist()

        print(f"Selected the top {len(current_feature_names)} features:")
        for feature in current_feature_names:
            print(f"- {feature}")

        # Reduce the datasets to only the selected features
        X_train_scaled = selector.transform(X_train_scaled)
        X_test_scaled = selector.transform(X_test_scaled)

    # --- Model Training (æ¨¡å‹è®­ç»ƒ) ---
    # Initialize the final model. `probability=True` is needed for ROC curve.
    model = SVC(kernel='linear', probability=True, random_state=42)
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    # --- Evaluation on Test Set (åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°) ---
    print("\n--- Test Set Evaluation Results ---")
    # MODIFIED: Added zero_division=0 to handle cases with no predicted samples for a class
    print(classification_report(y_test, y_pred, zero_division=0))

    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # --- Visualization (ç»“æœå¯è§†åŒ–) ---
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))
    RocCurveDisplay.from_estimator(model, X_test_scaled, y_test, ax=ax, name=model_name)
    ax.plot([0, 1], [0, 1], 'k--', label='Chance Level (AUC = 0.5)')
    ax.set_title(f'ROC Curve for {model_name}')
    ax.legend()
    plt.tight_layout()
    plt.show()

    # --- NEW: SHAP Value Calculation and Visualization (SHAPå€¼è®¡ç®—ä¸å¯è§†åŒ–) ---
    print("\n--- Calculating and Visualizing SHAP Values ---")
    # SHAP works best with a background dataset (we use the training data)
    # For linear models, shap.LinearExplainer is efficient.
    explainer = shap.LinearExplainer(model, X_train_scaled)
    shap_values = explainer.shap_values(X_test_scaled)

    # Create the SHAP summary plot
    print("Generating SHAP summary plot...")
    plt.title(f"SHAP Feature Importance for {model_name}")
    shap.summary_plot(shap_values, X_test_scaled, feature_names=current_feature_names, show=False)
    plt.tight_layout()
    plt.show()


# --- 4. Main Execution Block (ä¸»æ‰§è¡Œæ¨¡å—) ---

if __name__ == "__main__":
    # Load the data first
    df_main = load_data(FILE_PATH, SHEET_NAME)

    if df_main is not None:
        # ===================================================================
        # Task 1: Healthy Controls (HC) vs. All ADHD Classification
        # ===================================================================
        print("\n" + "=" * 50)
        print("ğŸš€ Task 1: Healthy Controls vs. All ADHD")
        print("=" * 50)

        df_task1 = df_main.copy()
        df_task1['ADHD_binary'] = df_task1['Group'].apply(lambda x: 0 if x == GROUP_HC else 1)
        X1, y1 = prepare_data(df_task1, FEATURE_COLUMNS, [0, 1], target_column='ADHD_binary')

        if X1 is not None and y1 is not None:
            # For this task, we use all features as it's already working well.
            run_svm_classification(X1, y1, model_name="HC vs. All ADHD", perform_feature_selection=False)
        else:
            print("Skipping Task 1 due to insufficient data.")

        # ===================================================================
        # Task 2: ADHD-I vs. ADHD-C Classification
        # ===================================================================
        print("\n" + "=" * 50)
        print("ğŸš€ Task 2: ADHD-I vs. ADHD-C Subtype Classification")
        print("=" * 50)

        X2, y2 = prepare_data(df_main, FEATURE_COLUMNS, [GROUP_ADHD1, GROUP_ADHD3], target_column='Group')

        if X2 is not None and y2 is not None:
            # MODIFIED: Now we enable feature selection to find the best 8 features
            # and generate SHAP plots to understand their impact.
            # You can change n_features_to_select to any number you want to test.
            run_svm_classification(
                X2, y2,
                model_name="ADHD-I vs. ADHD-C",
                perform_feature_selection=True,
                n_features_to_select=8
            )
        else:
            print("Skipping Task 2 due to insufficient data.")

    print("\nâœ… All tasks complete.")