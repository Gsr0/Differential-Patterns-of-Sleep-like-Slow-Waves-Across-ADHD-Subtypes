# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„SVMåˆ†ç±»æ¨¡å‹ - ADHDæ…¢æ³¢å’Œè¡Œä¸ºæ•°æ®åˆ†æ
Optimized SVM Classification Script for ADHD Slow Wave and Behavioral Data

ä¼˜åŒ–ç­–ç•¥:
1. ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹åŒ–
2. è¶…å‚æ•°è°ƒä¼˜
3. ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
4. é›†æˆå­¦ä¹ 
5. æ›´å¼ºçš„äº¤å‰éªŒè¯ç­–ç•¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.pipeline import Pipeline
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import warnings

warnings.filterwarnings('ignore')

# --- 1. é…ç½®å‚æ•° ---
FILE_PATH = 'antç½‘ç»œé›·è¾¾å›¾.xlsx'
SHEET_NAME = 'Sheet1'

# æ‰©å±•ç‰¹å¾åˆ—è¡¨
FEATURE_COLUMNS = [
    # 'Age',  # å¹´é¾„
    # 'Accuracy',  # è¡Œä¸ºå­¦ - å‡†ç¡®ç‡ (ACC)
    # 'Response Time',  # è¡Œä¸ºå­¦ - ååº”æ—¶ (RT)
    # 'Alerting',  # ANT - è­¦è§‰ç½‘ç»œ
    # 'Orienting',  # ANT - å®šå‘ç½‘ç»œ
    # 'Executive Control',  # ANT - æ‰§è¡Œæ§åˆ¶ç½‘ç»œ
    # 'maxnegpkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§è´Ÿæ³¢å¹…
    # 'maxnegpkamp_1',
    # 'maxnegpkamp_3',
    # 'maxnegpkamp_5',
    # 'maxnegpkamp_7',
    # 'maxnegpkamp_9',
    # 'maxnegpkamp_10',
    # 'maxnegpkamp_12',
    # 'maxnegpkamp_cluster1',
    # 'maxnegpkamp_cluster2',
    'maxnegpkamp_cluster',
    # 'maxnegpkamp/mxdnslp_1',
    # 'mxdnslp_1',
    # 'mxdnslp_3',
    # 'mxdnslp_4',
    # 'mxdnslp_5',
    # 'mxdnslp_7',
    # 'mxdnslp_8',
    # 'mxdnslp_9',
    # 'mxdnslp_12',
    # 'mxdnslp_14',
    # 'mxdnslp_18',
    # 'mxdnslp_cluster1',
    # 'mxdnslp_cluster2',
    'mxdnslp_cluster',
    'mxupslp_cluster',
    # 'mxupslp_1',
    # 'mxupslp_3',
    # 'mxupslp_7',
    # 'mxupslp_5',
    # 'maxpospkamp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§æ­£æ³¢å¹…
    # 'mxdnslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸‹é™æ–œç‡
    # 'mxupslp',  # æ…¢æ³¢å‚æ•° - æœ€å¤§ä¸Šå‡æ–œç‡
    # 'sw_density',  # æ…¢æ³¢å‚æ•° - æ…¢æ³¢å¯†åº¦
    # 'mean_duration'  # æ…¢æ³¢å‚æ•° - å¹³å‡æŒç»­æ—¶é—´
]

# å¯é€‰çš„é¢å¤–ç‰¹å¾ï¼ˆå¦‚æœæ•°æ®ä¸­å­˜åœ¨ï¼‰
OPTIONAL_FEATURES = [
    # 'Age',
    # 'Response Time',
    # 'Alerting',
    # 'Orienting',
    # 'Executive Control',
    # 'maxpospkamp',
    # 'mxupslp',
    # 'sw_density',
    # 'mean_duration'
]

GROUP_HC = 0
GROUP_ADHD1 = 1
GROUP_ADHD3 = 3


# --- 2. æ•°æ®åŠ è½½å‡½æ•° ---
def load_data(file_path, sheet_name):
    """åŠ è½½æ•°æ®å¹¶è¿›è¡ŒåŸºæœ¬æ£€æŸ¥"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: '{file_path}'")
        print(f"æ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"åˆ—å: {list(df.columns)}")
        return df
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° '{file_path}'")
        return None


def prepare_data_advanced(df, base_features, optional_features, target_groups, target_column='Group'):
    """é«˜çº§æ•°æ®å‡†å¤‡ï¼šç‰¹å¾å·¥ç¨‹å’Œé€‰æ‹©"""

    # ç­›é€‰ç›®æ ‡ç»„åˆ«
    df_filtered = df[df[target_column].isin(target_groups)].copy()

    # æ£€æŸ¥å“ªäº›å¯é€‰ç‰¹å¾åœ¨æ•°æ®ä¸­å­˜åœ¨
    available_features = base_features.copy()
    for feature in optional_features:
        if feature in df_filtered.columns:
            available_features.append(feature)
            print(f"âœ… æ·»åŠ ç‰¹å¾: {feature}")

    # åˆ é™¤ç¼ºå¤±å€¼
    df_filtered.dropna(subset=available_features + [target_column], inplace=True)

    if len(df_filtered) < 10:
        print(f"âš ï¸ è­¦å‘Š: æ•°æ®ä¸è¶³ï¼Œä»…æœ‰ {len(df_filtered)} ä¸ªæ ·æœ¬")
        return None, None, None

    # ç‰¹å¾å·¥ç¨‹ï¼šåˆ›å»ºæ–°ç‰¹å¾
    print("\nğŸ”§ ç‰¹å¾å·¥ç¨‹...")
    feature_df = df_filtered[available_features].copy()

    # åˆ›å»ºæ¯”ç‡ç‰¹å¾
    if 'maxnegpkamp_cluster1' in feature_df.columns and 'maxnegpkamp_cluster2' in feature_df.columns:
        feature_df['cluster_ratio'] = feature_df['maxnegpkamp_cluster1'] / (feature_df['maxnegpkamp_cluster2'] + 1e-8)

    if 'mxdnslp_cluster1' in feature_df.columns and 'mxdnslp_cluster2' in feature_df.columns:
        feature_df['slope_ratio'] = feature_df['mxdnslp_cluster1'] / (feature_df['mxdnslp_cluster2'] + 1e-8)

    # åˆ›å»ºäº¤äº’ç‰¹å¾
    if 'Accuracy' in feature_df.columns and 'maxnegpkamp' in feature_df.columns:
        feature_df['acc_amplitude_interaction'] = feature_df['Accuracy'] * feature_df['maxnegpkamp']

    # ç›®æ ‡å˜é‡
    y = df_filtered[target_column].map({target_groups[0]: 0, target_groups[1]: 1})

    print(f"æœ€ç»ˆç‰¹å¾æ•°: {len(feature_df.columns)}")
    print(f"æ ·æœ¬æ•°: {len(feature_df)}")
    print(f"ç±»åˆ«åˆ†å¸ƒ:\n{y.value_counts()}")

    return feature_df, y, list(feature_df.columns)


# --- 3. ä¼˜åŒ–çš„SVMæ¨¡å‹è®­ç»ƒå‡½æ•° ---
def run_optimized_svm_classification(X, y, feature_names, model_name="Optimized SVM"):
    """è¿è¡Œä¼˜åŒ–çš„SVMåˆ†ç±»"""

    print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ– {model_name}")
    print("=" * 50)

    # 1. ç‰¹å¾é€‰æ‹©
    print("1ï¸âƒ£ ç‰¹å¾é€‰æ‹©...")

    # ä½¿ç”¨SelectKBestè¿›è¡Œç‰¹å¾é€‰æ‹©
    k_best = min(len(feature_names), max(3, len(feature_names) // 2))
    selector = SelectKBest(score_func=f_classif, k=k_best)
    X_selected = selector.fit_transform(X, y)

    selected_features = [feature_names[i] for i in selector.get_support(indices=True)]
    print(f"é€‰æ‹©çš„ç‰¹å¾: {selected_features}")

    # 2. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    print("2ï¸âƒ£ å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")

    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    class_counts = pd.Series(y).value_counts()
    print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(class_counts)}")

    # å¦‚æœç±»åˆ«ä¸å¹³è¡¡ä¸¥é‡ï¼Œä½¿ç”¨SMOTE
    if min(class_counts) / max(class_counts) < 0.7:
        print("ä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·...")
        smote = SMOTE(random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X_selected, y)
        print(f"å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ: {pd.Series(y_balanced).value_counts().to_dict()}")
    else:
        X_balanced, y_balanced = X_selected, y

    # 3. è¶…å‚æ•°è°ƒä¼˜
    print("3ï¸âƒ£ è¶…å‚æ•°è°ƒä¼˜...")

    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grid = {
        'C': [0.1, 1, 10, 100],
        'kernel': ['linear', 'rbf'],
        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
    }

    # ä½¿ç”¨ç½‘æ ¼æœç´¢
    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    svm_grid = GridSearchCV(
        SVC(probability=True, random_state=42),
        param_grid,
        cv=cv_strategy,
        scoring='f1',
        n_jobs=-1
    )

    svm_grid.fit(X_balanced, y_balanced)

    print(f"æœ€ä½³å‚æ•°: {svm_grid.best_params_}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯F1å¾—åˆ†: {svm_grid.best_score_:.3f}")

    # 4. é›†æˆå­¦ä¹ 
    print("4ï¸âƒ£ æ„å»ºé›†æˆæ¨¡å‹...")

    # åˆ›å»ºå¤šä¸ªæ¨¡å‹
    svm_best = svm_grid.best_estimator_
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    lr_model = LogisticRegression(random_state=42, max_iter=1000)

    # æŠ•ç¥¨åˆ†ç±»å™¨
    ensemble = VotingClassifier(
        estimators=[
            ('svm', svm_best),
            ('rf', rf_model),
            ('lr', lr_model)
        ],
        voting='soft'
    )

    # 5. æ¨¡å‹è¯„ä¼°
    print("5ï¸âƒ£ æ¨¡å‹è¯„ä¼°...")

    # ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œè¯„ä¼°ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X_selected, y, test_size=0.25, random_state=42, stratify=y
    )

    # ç¼©æ”¾ç‰¹å¾
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # è®­ç»ƒæ¨¡å‹
    ensemble.fit(X_train_scaled, y_train)

    # é¢„æµ‹
    y_pred = ensemble.predict(X_test_scaled)
    y_pred_proba = ensemble.predict_proba(X_test_scaled)[:, 1]

    # è¯„ä¼°ç»“æœ
    print("\nğŸ“Š é›†æˆæ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(classification_report(y_test, y_pred))
    print("\næ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(y_test, y_pred))

    # AUCå¾—åˆ†
    if len(np.unique(y_test)) > 1:
        auc_score = roc_auc_score(y_test, y_pred_proba)
        print(f"\nAUCå¾—åˆ†: {auc_score:.3f}")

    # 6. å•ç‹¬è¯„ä¼°SVMæ¨¡å‹
    print("\nğŸ“Š ä¼˜åŒ–SVMæ¨¡å‹è¯„ä¼°ç»“æœ:")
    svm_pred = svm_best.predict(X_test_scaled)
    print(classification_report(y_test, svm_pred))

    # 7. äº¤å‰éªŒè¯è¯„ä¼°
    print("\nğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
    cv_scores = cross_val_score(ensemble, X_selected, y, cv=cv_strategy, scoring='f1')
    print(f"é›†æˆæ¨¡å‹F1å¾—åˆ†: {np.mean(cv_scores):.3f} (Â± {np.std(cv_scores):.3f})")

    cv_scores_svm = cross_val_score(svm_best, X_selected, y, cv=cv_strategy, scoring='f1')
    print(f"SVMæ¨¡å‹F1å¾—åˆ†: {np.mean(cv_scores_svm):.3f} (Â± {np.std(cv_scores_svm):.3f})")

    # 8. ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§åˆ†æ:")
    if hasattr(ensemble.named_estimators_['rf'], 'feature_importances_'):
        feature_importance = ensemble.named_estimators_['rf'].feature_importances_
        importance_df = pd.DataFrame({
            'feature': selected_features,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        print(importance_df)

    return ensemble, svm_best, selected_features


# --- 4. ä¸»æ‰§è¡Œå‡½æ•° ---
def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # åŠ è½½æ•°æ®
    df_main = load_data(FILE_PATH, SHEET_NAME)

    if df_main is None:
        return

    # ä»»åŠ¡1: HC vs All ADHD
    print("\n" + "=" * 60)
    print("ğŸ¯ ä»»åŠ¡1: å¥åº·å„¿ç«¥ vs æ‰€æœ‰ADHD")
    print("=" * 60)

    df_task1 = df_main.copy()
    df_task1['ADHD_binary'] = df_task1['Group'].apply(lambda x: 0 if x == GROUP_HC else 1)

    X1, y1, features1 = prepare_data_advanced(
        df_task1, FEATURE_COLUMNS, OPTIONAL_FEATURES, [0, 1], 'ADHD_binary'
    )

    if X1 is not None:
        ensemble1, svm1, selected_features1 = run_optimized_svm_classification(
            X1, y1, features1, "HC vs All ADHD"
        )

    # ä»»åŠ¡2: ADHD-I vs ADHD-C
    print("\n" + "=" * 60)
    print("ğŸ¯ ä»»åŠ¡2: ADHD-I vs ADHD-C")
    print("=" * 60)

    X2, y2, features2 = prepare_data_advanced(
        df_main, FEATURE_COLUMNS, OPTIONAL_FEATURES, [GROUP_ADHD1, GROUP_ADHD3], 'Group'
    )

    if X2 is not None:
        ensemble2, svm2, selected_features2 = run_optimized_svm_classification(
            X2, y2, features2, "ADHD-I vs ADHD-C"
        )

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("1. å¦‚æœå¯èƒ½ï¼Œæ”¶é›†æ›´å¤šæ•°æ®æ ·æœ¬")
    print("2. è€ƒè™‘ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚æœæ•°æ®é‡è¶³å¤Ÿï¼‰")
    print("3. è¿›è¡Œæ›´å¤šç‰¹å¾å·¥ç¨‹ï¼Œå¦‚æ—¶é—´åºåˆ—ç‰¹å¾")
    print("4. è€ƒè™‘ä½¿ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•")


if __name__ == "__main__":
    main()